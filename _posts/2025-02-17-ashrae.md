---
layout: post
title: "ASHRAE III"
date: 2025-02-17
categories: blog
---

## Introduction

This blog post summarises some of my work on the [ASHRAE III energy prediction competition](https://www.kaggle.com/competitions/ashrae-energy-prediction). The aim of this competition was to predict energy consumption for a range of different buildings, across several sites and meter types in order to estimate the energy efficiency gains and cost savings from any building retrofits. Knowing how much energy a building would have consumed without any improvements is challenging but is necessary in order to quantify efficiency improvements. One option is to build a counterfactual model that estimates building energy consumption in the absence of any building improvements, and the purpose of this competition was to find out if machine learning can help with this task.

The code for all plots, feature engineering, and models can be found [here](https://github.com/lmalms/sequence-learners/tree/main/ashrae).

## Data

The training dataset consisted of hourly meter readings for the year 2016 for four different meter types (chilled water, electricity, hot water, and steam) from roughly 1450 buildings across 15 different sites. Note that not every building had readings for every meter type. In addition to the readings data, the training data contained metadata about each building such as its primary use (e.g. Education, Office, Retail, etc.), its size and floor count, as well as weather data from each site.

## Evaluation

Models were evaluated by comparing model predictions against ground truth meter readings for 2017, and 2018. Predictions were evaluated using RMSLE between predicted and actual meter readings. This metric has some useful properties that make it a suitable evaluation metric for this competition:

-> Add link to notebook here

### Robustness to outliers

The RMSLE is more stable in the presence of outliers compared to for example the RMSE. As shown later on, the training data (and therefore likely also the evaluation data) contained several short spikes in consumption data, some of them up to 100x of the average meter reading. Those are likely erroneous readings rather than true consumption data and therefore the model should not be penalised if it gets those measurements wrong. However, the RMSE would explode in the presence of these measurements and the total error would likely be dominated by these outliers, while the RMSLE is relatively stable.

### Sensitivity to relative errors

The RMSLE is sensitive to relative errors. That is predicting 10.1 for a true value of 10.0 is penalised (roughly) the same as predicting 101 for a true value of 100. The relative error is 1% in both cases but the absolute error differs by a factor of 10. The RMSE on the other hand is not sensitive to the scale of the error relative to the true value. In the example above, the error would differ by 10x between the two predictions.

This sensitivity to relative errors is a useful property for this use case as we are trying to estimate efficiency gains for individual buildings which is a relative measure. If we penalised errors for buildings with high average consumption more strongly than errors on buildings with low consumption, we would bias model selection towards models that performed well on high consumption buildings but that might produce poor estimates for buildings with low consumption.

### Asymmetry

## Data, feature engineering, outliers

## Modelling

## Results, error analysis

## Learnings

## Resources

- [Post on the differences between RMSLE vs. RMSE metrics](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)
