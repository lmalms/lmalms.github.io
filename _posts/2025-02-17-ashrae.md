---
layout: post
title: "ASHRAE III"
date: 2025-02-17
categories: blog
---

## Introduction

This blog post summarises some of my work on the [ASHRAE III energy prediction competition](https://www.kaggle.com/competitions/ashrae-energy-prediction). The aim of this competition was to predict energy consumption for a range of different buildings, across several sites and meter types in order to estimate the energy efficiency gains and cost savings from building retrofits. Knowing how much energy a building would have consumed without any improvements is challenging but is necessary in order to quantify efficiency improvements. One option is to build a counterfactual model that estimates building energy consumption in the absence of any building improvements, and the purpose of this competition was to find out if machine learning can help with this task.

The code for all plots, feature engineering, and models can be found [here](https://github.com/lmalms/sequence-learners/tree/main/ashrae).

## Data

The training dataset consisted of hourly meter readings for the year 2016 for four different meter types (chilled water, electricity, hot water, and steam) from roughly 1450 buildings across 15 different sites. Note that not every building had readings for every meter type. In addition to the readings data, the training data contained metadata about each building such as its primary use (e.g. Education, Office, Retail, etc.), its size and floor count, as well as weather data from each site.

## Evaluation

Models were evaluated by comparing model predictions against ground truth meter readings for 2017, and 2018. Predictions were evaluated using the root mean squared logarithmic error (RMSLE) between predicted and actual meter readings. This metric has some interesting properties that make it a useful evaluation metric for this competition:

-> Add link to notebook here

### Robustness to outliers

The RMSLE is more stable in the presence of outliers compared to for example the RMSE. As shown later on, the training data (and therefore likely also the evaluation data) contained several short spikes in consumption data, some of them up to 100x of the average meter reading. Those are likely erroneous readings rather than true consumption data and therefore the model should not be penalised for getting those measurements wrong. However, the RMSE would explode in the presence of these measurements and the total error would likely be dominated by these outliers, while the RMSLE is relatively stable.

### Sensitivity to relative errors

The RMSLE is sensitive to relative errors. That is predicting 10.1 for a true value of 10.0 is penalised (roughly) the same as predicting 101 for a true value of 100. The relative error is 1% in both cases but the absolute error differs by a factor of 10. The RMSE on the other hand is not sensitive to the scale of the error relative to the true value. In the example above, the error would differ by 10x between the two predictions.

This sensitivity to relative errors is a useful property for this use case as we are trying to estimate efficiency gains for individual buildings which is a relative measure. If we penalised errors for buildings with high average consumption more strongly than errors on buildings with low consumption, we would bias model selection towards models that performed well on high consumption buildings but that might produce poor estimates for buildings with low consumption.

### Asymmetric penalty

RMSLE penalises underestimation more strongly than overestimation of the true value. This is an interesting property and I can see how in some business cases a slight overestimation can be tolerated while underestimation cannot. However, for this particular use case it is unclear to me which way the asymmetry should really go. From the perspective of encouraging building owners to invest in retrofits, underestimating efficiency gains would make efficiency gains look smaller than they actually are hence discouraging buildings owners to make the necessary investments. However, from the perspective of reporting on existing retrofits and estimating how much energy has actually been saved, overestimation would make the efficiency gains seem larger than they actually are.

## Data, outliers, and feature engineering

### Energy consumption patterns

-> Link to notebook

The plot below shows a timeseries of energy consumption (log transformed) by meter type for each building in the training dataset (gray). It also show the average consumption across all buildings for that meter type (orange). Although there can be some large variation in consumption for individual buildings, there are some patterns in the average consumption data across all buildings. For electricity for example, there is a clear weekly pattern with higher consumption during the week compared to the weekend. For chilled water, there is a clear seasonal pattern with higher consumption in the summer compared to the winter as well as a weaker weekly pattern. For steam and hot water, there is the longer term seasonal pattern of lower consumption during summer months but no clear weekly pattern.

![meter-readings-by-type](/assets/ashrae/meter_readings_by_type.png)
*Energy consumption by meter type for each building in the training dataset (gray) and average consumption across all buildings for that meter type (orange).*

The plot below shows a heatmap of average energy consumption for each meter type broken down by building use. It is clear that a building's primary use has a large impact on its energy consumption. The seasonal patterns mentioned above are again visible but there are also some differences in consumption patterns between different building types. For example, while most buildings show the weekly pattern of higher electricity consumption during the week, the pattern is less pronounced for buildings in the Warehouse/Storage and Technology/Science categories. Buildings in the Warehouse/Storage category also show lower electricity consumption compared to other building types on average. As another example, even though most buildings show the seasonal pattern of reduced chilled water consumption in the winter, buildings in the Utility category show nearly constant very high consumption throughout the year.

![meter-readings-by-use](/assets/ashrae/meter_readings_by_type_and_use_heatmap.png)
*Average energy consumption by meter type and building use.*

### Outliers

The plot below highlights some of the type of outliers and anomalies present in the training data. The plot focusses on anomalies in the electricity meter readings but similar types of anomalies were present in the other meter types as well. I would broadly classify the anomalies into four categories:

1. **Prolonged periods of zero consumption (top plot)**: Some buildings have prolonged periods of zero consumption. Whether this is an anomaly or not likely depends on the meter type. For electricity for example it seems unlikely to have long periods of zero consumption, while for chilled water it might be more common to have zero consumption during the winter months (see the heatmap above).

2. **Prolonged periods of constant non-zero consumption (second plot)**: Some buildings have prolonged periods of constant non-zero consumption. I often saw periods of constant non-zero consumption for 24 hours or more. This again seemed unlikely to me.

3. **Short spikes in consumption (third plot)**: Some buildings have short spikes in consumption. These spikes could be up to 100x the average consumption for that building, which again seemed unlikely.

4. **Consumption shifts (bottom plot)**: Some buildings have sudden upward / downward shifts in consumption, even though the hourly consumption pattern remained the same.

![outliers](/assets/ashrae/outlier_types_electricity.png)
*Different types of outliers in electricity meter readings.*

I removed the first two types of anomalies from the training data as they were relatively easy to spot, and it was possible to write some simple logic to identify and remove them. The third and fourth type of anomalies were more difficult to spot and I did not remove them from the training data. Future work could focus on developing more automated methods (e.g. anomaly detection algorithms) to identify and remove these types of anomalies.

It is worth pointing out that removing these anomalies was the key to getting good predictions in this competition.

### Feature engineering

-> Link to notebook

I included all of the features provided in the training data. I also added the following basic features:

**Temporal features (sine and cosine)**: Hour of the day, day of the week, month of the year

**Building metadata**: Building age, building size (square feet * floor count)

**Weather data**: Feels-like temperature, cold and heat index, lagged and rolling average temperature and pressure features

I did not add any lagged features because there were often gaps in the training data, either because the data was missing from the start or was removed as an outlier, which meant that the lagged features would also often be missing. The forecasting horizon was also very long (two years) and I wasn't sure how well and auto-regressive approach would work over such a long horizon. Instead I assumed a more direct regression approach would work better.

I tried target encoding the high cardinality building id feature but this did not improve model performance. Adding holiday periods would likely have improved predictions but it was unclear which sites the buildings were located in (only an integer site id was provided) and therefore which holidays to include.

## Modelling

### Baseline model

### LightGBM

## Results, error analysis

## Learnings

## Future work

## Resources

- [Post on the differences between RMSLE vs. RMSE metrics](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)
- Links to code
- Links to target encoding
